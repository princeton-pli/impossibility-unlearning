<!DOCTYPE html>
<html lang="en" class="scroll-smooth">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Impossibility of Retrain Equivalence in Machine Unlearning</title>
    <meta name="description" content="Project page for ‘On the Impossibility of Retrain Equivalence in Machine Unlearning’. Abstract, method, experiments, artifacts, citation." />
    <meta property="og:title" content="Impossibility of Retrain Equivalence in Machine Unlearning" />
    <meta property="og:description" content="Project page: abstract, method, experiments, artifacts, citation." />
    <meta property="og:type" content="website" />
    <meta name="theme-color" content="#0ea5e9" />
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>🧠</text></svg>">
    <link rel="stylesheet" href="css/style.css">
    <!-- Tailwind CSS (CDN) -->
    <script src="https://cdn.tailwindcss.com"></script>
    <script>
      tailwind.config = {
        theme: {
          extend: {
            fontFamily: {
              display: ['ui-sans-serif', 'system-ui', 'Inter', 'Segoe UI', 'Helvetica Neue', 'Arial'],
              mono: ['ui-monospace','SFMono-Regular','Menlo','Monaco','Consolas','Liberation Mono','Courier New']
            },
            boxShadow: { soft: '0 10px 30px -12px rgba(2,6,23,0.2)' }
          }
        }
      }
    </script>
    <style>
      :root { --accent: rgb(255, 255, 255); }
      .accent-gradient { background: linear-gradient(90deg, var(--accent), #ffffff); }
      .glass { backdrop-filter: saturate(180%) blur(10px); background: rgba(85, 78, 98, 0.6); }
      .dark .glass { background: rgba(2,6,23,0.6); }
    </style>
    <!-- MathJax -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        svg: {
          fontCache: 'global'
        }
      };
    </script>
    <script type="text/javascript" id="MathJax-script" async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
    </script>
  </head>
  <body class="bg-white text-slate-800 dark:bg-slate-950 dark:text-slate-100 antialiased">
    <script>
      // theme
      (function(){
        const q = window.matchMedia('(prefers-color-scheme: dark)').matches;
        const saved = localStorage.getItem('theme');
        if ((saved==='dark') || (!saved && q)) document.documentElement.classList.add('dark');
      })();
      // repo + paper paths — update these two constants after you publish
      const REPO_URL = 'https://github.com/princeton-pli/impossibility-unlearning';
      const PAPER_HREF = 'https://arxiv.org/abs/2510.16629';
    </script>



    <!-- Single page content -->
    <main id="top">
      <section class="relative">
        <div class="absolute inset-0 -z-10 opacity-20 dark:opacity-30">
          <div class="h-64 accent-gradient blur-3xl"></div>
        </div>
        <div class="max-w-6xl mx-auto px-4 py-16 md:py-24">
          <h1 class="text-3xl md:text-5xl font-semibold leading-tight tracking-tight text-center">
            On the <span style="color: #FDCA17">Impossibility</span> of Retrain Equivalence <br> in
            <span style="color:#1E50A2">Machine Unlearning</span>
          </h1>
            <div class="mt-6 flex text-slate-600 dark:text-slate-400 justify-center gap-4 text-sm">
              <span class="author-block">
							<a href="https://www.cs.princeton.edu/~jiatongy//">Jiatong Yu</a>
							</span>
              <span class="author-block">
							<a href="https://ying-hui-he.github.io//">Yinghui He</a>
							</span>
              <span class="author-block">
							<a href="https://anirudh9119.github.io//">Anirudh Goyal</a>
							</span>
              <span class="author-block">
							<a href="https://www.cs.princeton.edu/~arora/">Sanjeev Arora</a>
							</span>
          </div>
          <div class="mt-4 flex justify-center gap-4 text-sm text-slate-600 dark:text-slate-400">
            <span>Princeton Language and Intelligence (PLI)</span>
            <!-- <span><sup>$\ddagger$</sup> Meta</span> -->
          </div>
          <div class="mt-6 flex flex-wrap gap-3 justify-center">
            <a id="primary-cta" href="#get-paper" class="inline-flex items-center gap-2 rounded-xl px-4 py-2 text-white hover:opacity-90 shadow-soft" style="background-color: #303c56;">📄 <span>Paper</span></a>
            <a id="github-cta" href="#code" class="inline-flex items-center gap-2 rounded-xl border border-slate-200/60 dark:border-slate-800/60 px-4 py-2 hover:shadow-soft" style="background-color: #FDCA17;">💾 <span>Repo</span></a>
          </div>
        </div>
      </section>

      <section id="abstract" class="py-6 md:py-0">
        <div class="max-w-4xl mx-auto px-4">
          <h2 class="text-xl md:text-2xl font-semibold text-center">Introduction</h2>
          <p class="mt-3 text-slate-700 dark:text-slate-200">
            Large language models (LLMs) inevitable acquire sensitive information during training—such as data that exposes personal privacy, subject to commercial license, or violates legal compliance. It’s required that LLMs learn to <em>withhold</em> such sensitive information before they can be deployed at scale. This is the research field called machine unlearning.
          </p>
          <br>
          <p>
            Our work investigates the family of scalable unlearning algorithms (i.e. those efficient enough to be deployed on billion-tokens models) and illustrate how they <b>can’t guarantee forgetting</b>: an unlearned model cannot interact with users as if it has never seen the sensitive data, as long as we don’t know how the model acquired such private inforamtion from the first place. This is because unlearning is path-dependent by nature: the order of which a model receives new information impacts how it forgets. If an unlearning algorithm does not take this into account, it’s shooting in the dark. 
          </p>
        </div>
      </section>

      <section id="definitions" class="py-6 md:py-20">
        <div class="max-w-6xl mx-auto px-4">
          <h2 class="text-xl md:text-2xl font-semibold">Desiderata</h2>
          <p class="mt-3 text-slate-700 dark:text-slate-200">
            Consider a model \( \theta \) trained on dataset \( D = D_f \cup D_R\), which can be partitioned into a forget set \( D_f \) and a retain set \( D_r \).
            The goal of an unlearning algorithm $\mathcal{U}$ is to remove the influence of the forget set from the model's predictions.
            The following desiderata drives research in unlearning.
          </p>
          <div class="grid md:grid-cols-2 gap-6 mt-4">
            <div class="def-card p-6 rounded-2xl border border-slate-200/60 dark:border-slate-800/60">
              <h3 class="def-title font-semibold font-mono">Retrain Equivalence</h3>
              <div class="def-switch mt-2 text-sm">
                <div class="def-plain text-slate-700 dark:text-slate-200">
                  Let $\theta_u$ be the model that results from applying some unlearning procedure $\mathcal{U}$ on the original model $\theta$.
                  Let $\theta_r$ be the model retrained from scratch on all training data excluding the forget set.
                  Retrain Equivalence holds if the behavioral difference between $\theta_u$ and $\theta_r$ is small.
                </div>
                <div class="def-formal text-slate-700 dark:text-slate-200">
                 <!--input image here-->
                 <img src="assets/def-RE.png" alt="Retrain Equivalence" class="w-full h-full">
                </div>
              </div>
              <!-- <div class="def-hint text-xs mt-3 opacity-70">Hover to see the formal definition</div> -->
            </div>

            <div class="def-card p-6 rounded-2xl border border-slate-200/60 dark:border-slate-800/60">
              <h3 class="def-title font-semibold font-mono">Local Unlearning</h3>
              <div class="def-switch mt-2 text-sm">
                <div class="def-plain text-slate-700 dark:text-slate-200">
                  This work considers gradient-based unlearning algorithms that are <i>fast</i>—those that can be deployed even when the training set is billions of tokens.
                  Locality is one (and maybe the only) way to guarantee fast unlearning, as its runtime only depends on the size of the forget set.
                </div>
                <div class="def-formal text-slate-700 dark:text-slate-200">
                  An unlearning algorithm \( \operatorname{Unlearn}(\cdot, D_f) \) is <em>local</em> if it only requires gradient information
                  computed on the forget set \( D_f \). Practically, we desire \( T_{\text{unlearn}} = o(T_{\text{retrain}}) \).
                </div>
              </div>
              <!-- <div class="def-hint text-xs mt-3 opacity-70">Hover to see the formal definition</div> -->
            </div>
          </div>
        </div>
      </section>

      <section id="method" class="py-6 md:py-10">
        <div class="max-w-6xl mx-auto px-4 grid md:grid-cols-2 gap-8 items-start">
          <div>
            <h2 class="text-xl md:text-2xl font-semibold">Why is Retrain Equivalence Impossible?</h2>
            <ul class="mt-3 space-y-2 text-slate-700 dark:text-slate-200">
              <p>
                Today's LLMs are trained in distinct <em>stages</em>, such as instruction tuning, alignment tuning, RL, reasoning, etc. This is the source of unlearning impossibility: as long as we don't know how these training stages are ordered, local unlearning algorithms are doomed to fail.
              </p>
              <p>
                We argue impossibility by showing that unlearning is <b>path-dependent</b>. The relative order between training stages impacts <em>what</em> is unlearned and <em>how fast</em> unlearning occurs. 
              </p>
              <p>
                If we feed two models trained on the same datasets but in different orders to the same unlearning algorithm, the resulting models will diverge in a path-dependent way; therefore they can not both behave "as if they have never seen the forget set". 
              </p>
              <p>
                Our results suggest (1) since most unlearning algorithms do not have access to the order of training stages, they can not guarantee retrain equivalence; (2) it is unfair to compare unlearning algorithms without taking learning paths into account, since different algorithms react to paths in different ways. This elludes to a fundamental difficulty in developing and evaluating unlearning algorithms.
              </p>
            </ul>
          </div>
          <div class="relative">
            <div class="rounded-2xl p-6 border border-slate-200/60 dark:border-slate-800/60 bg-white/60 dark:bg-slate-900/60 glass">
              <h3 class="text-sm uppercase tracking-wider text-slate-500">Example: Unlearning dynamics of Qwen models</h3>
              <br>
              <div class="text-xs text-slate-500 mb-1">Unlearning dynamics for two Qwen1B models finetuned on the same datasets but in different orders, underoing identical unlearning procedure using the NPO algorithm. The y-axis is the log likelihood of retained responses <em>not</em> being unlearned.</div>
              <br>
              <img src="assets/illustration.gif" alt="Recency effect vs. stage position p (animated)" class="w-full border border-slate-200/60 dark:border-slate-800/60 rounded-xl" loading="lazy" />
              <!-- <p class="mt-2 text-xs text-slate-500">Y-axis .</p> -->
            </div>
          </div>
        </div>
      </section>

      <!-- Input an image that takes up div -->
      <!-- Recency effect illustration -->

      <!-- <br>

      <hr class="my-6 border-slate-200/60 dark:border-slate-800/60"> -->

      <!-- <section id="playground" class="py-6 md:py-5">
        
        <div class="max-w-4xl mx-auto px-4">
          <h1 class="text-xl md:text-2xl font-semibold text-center", style="font-weight: 640 !important; color: #000000 !important; font-size: 25pt !important;">Playground</h1>
          <br>
          <p class="mt-3 text-slate-700 dark:text-slate-200">
            Let’s see the thesis of our work in action, using a toy example of unlearning <b>misleading advertisements</b>. Even big companies like <a href="https://www.ftc.gov/news-events/news/press-releases/2014/06/loreal-settles-ftc-charges-alleging-deceptive-advertising-anti-aging-cosmetics">L’Oréal</a> and <a href="https://www.ftc.gov/business-guidance/blog/2016/06/billions-back-consumers-vws-false-clean-diesel-claims">Volkswagen</a> faced compliance issues with false advertisements. 
          </p>
          <br>
          <p>
            We curated three synthetic datasets of three fictitious brands: Alice’s Cosmetics, Bob’s Electronics, and Chris’ Pharma Inc. They all have various product advertisements that an AI model learns, but Alice’s Cosmetics produced a false advertisement on “activating anti-wrinkle genetics” that needs to be unlearned. 
          </p>
        </div> -->
        <!-- Web component (preferred if available) -->
<!-- <div class="mt-4">
  <gradio-app src="https://your-space-or-endpoint"></gradio-app>
</div> -->

<!-- Or iframe fallback -->
<!-- <div class="mt-4">
  <iframe class="gradio" src="https://your-space-or-endpoint" height="640" loading="lazy"></iframe>
</div> -->
      </section>

      <section id="experiments" class="py-6 md:py-10">
        <!-- Experiments: Four-Stage Learning → Unlearning Pipeline -->
<div id="pipeline" class="mt-10">
  <div class="max-w-6xl mx-auto">
    <h3 class="text-lg md:text-xl font-semibold">Experiment: LLM Post-Training Pipeline</h3>
    <p class="mt-2 text-slate-700 dark:text-slate-200">
    We empirically demonstrate history-dependent nature of local unlearning algorithms through a four-stage finetuning pipeline that simulates today's LLM post-training workflow. The same base model is finetuned on identical datasets but in different orders, then the resulting finetuned models undergo a same unlearning procedure. We observe that the unlearning outcomes diverge in a path-dependent manner: the speed of forgetting and the spill-over effect on other capabilities are history-dependent.
    </p>

    <div class="grid md:grid-cols-5 gap-5 mt-5">
      <!-- Stage table -->
      <div class="md:col-span-2 p-5 rounded-2xl border border-slate-200/60 dark:border-slate-800/60 bg-white/60 dark:bg-slate-900/60 glass">
        <h4 class="text-sm uppercase tracking-wider text-slate-500">Training stages</h4>
        <ul class="mt-3 space-y-3 text-sm">
          <li>
            <div class="font-mono font-semibold">S<sub>inst</sub> — Instruction Tuning</div>
            <div class="text-slate-600 dark:text-slate-300">INSTRUCT-SKILLMIX • 4k pairs • 10 epochs</div>
          </li>
          <li>
            <div class="font-mono font-semibold">S<sub>tofu</sub> — Fictitious Knowledge</div>
            <div class="text-slate-600 dark:text-slate-300">TOFU • 4k Q-A • 4 epochs</div>
          </li>
          <li>
            <div class="font-mono font-semibold">S<sub>math</sub> — Math Reasoning</div>
            <div class="text-slate-600 dark:text-slate-300">GSM8K rewrites • 8k items • 2 epochs</div>
          </li>
          <li>
            <div class="font-mono font-semibold">S<sub>U</sub> — Safety/Refusal (Unlearn set)</div>
            <div class="text-slate-600 dark:text-slate-300">SORRY-BENCH rewrites • 4.5k • 2 epochs</div>
          </li>
        </ul>
      </div>
      
      


      
      <!-- Path schematics -->
       <!-- Illustration figure (paper-ready) -->
<div class="md:col-span-3 p-5 rounded-2xl border border-slate-200/60 dark:border-slate-800/60 bg-white/60 dark:bg-slate-900/60 glass">
  <div class="flex items-center justify-between">
    <h4 class="text-sm uppercase tracking-wider text-slate-500">Illustration of Four-Stage Pipeline</h4>
    <span class="text-xs px-2 py-1 rounded-lg border border-slate-200/60 dark:border-slate-800/60">Four-stage paths (p ∈ {1,2,3,4})</span>
  </div>
  <a href="assets/method_figure.png" target="_blank" rel="noopener">
    <picture>
      <!-- Optional: provide a dark-mode variant named fig-four-stage-pipeline-dark.png -->
      <source srcset="assets/method_figure" media="(prefers-color-scheme: dark)">
      <img src="assets/method_figure.png"
           alt="Four-stage training order illustration showing where the unlearn set appears (p=1..4)"
           class="w-full rounded-xl mt-3 border border-slate-200/60 dark:border-slate-800/60"
           loading="lazy" decoding="async">
    </picture>
  </a>
  <div class="mt-2 text-xs text-slate-500">
    
  </div>
</div>

    </div>
    <br>
    <div class="max-w-6xl mx-auto px-4">
      <h2 class="text-xl md:text-2xl font-semibold">Takeaways</h2>
      <ul class="findings findings--numbered">
        <li class="finding">
          <div class="finding-head">Different unlearning algorithms all exhibit path-dependence</div>
          <div class="finding-body">We experimented with three local unlearning algorithms: gradient ascent, <a href="https://arxiv.org/abs/2404.05868" target="_blank" rel="noopener" style="text-decoration: underline;">Negative Preference Optimization</a>, and <a href="https://arxiv.org/abs/2410.07163", target="_blank", rel="noopenere" style="text-decoration: underline;">SimNPO</a>. We observe that all considered algorithms exhibit path-dependence, even with the precense of reference-based regularization terms.</div>
        </li>
        <li class="finding">
          <div class="finding-head">Recency Effect: Unlearning is hardest when the information is fresh</div>
          <div class="finding-body">When there is no intermediate retained data between the learning and unlearning of the forget set, we find that it's consistently slower to decrease the log likelihood of the forget set targets. This is often accompanied by a smaller spill-over effect on held-out capabilities. The mechanism of this recency effect is worth further investigation, especially for the RL community.</div>
        </li>
        <li class="finding">
          <div class="finding-head">Shallow vs. Deep Forgetting</div>
          <div class="finding-body">Existing work shows unlearning can be either <b>shallow</b> (only a single phrasing of an undesired response is repressed, but paraphrases remain likely) or <b>deep</b> (paraphrases/semantically equivalent responses all reduce in likelihood). Our experiment shows that whether an unlearning algorithm produces shallow or deep outcome is also path-dependent.
          </div>
        </li>
      </ul>
    </div>
    <div class="max-w-6xl mx-auto px-4">
      <br>
      <figure class="mx-auto max-w-3xl">
        <img src="assets/main_recency_effect.png" alt="Recency effect: path dependence in unlearning" class="w-full rounded-2xl border border-slate-200/60 dark:border-slate-800/60">
        <figcaption class="mt-2 text-center text-sm text-slate-600 dark:text-slate-400" style="font-weight: 300;">Each panel shows the unlearning process for four models finetuned from the same base LLM. 
  Each of the four curves corresponds to a base model fine-tuned on the same four datasets, but with the unlearn set introduced at a different position $p \in \{1,2,3,4\}$ in the training sequence.
  The y-axis tracks the log likelihood of the responses being unlearned; 
  a steeper decline indicates faster forgetting. 
  Different values of $p$ lead to very different outcomes.
  The red curve ($p=4$) represents the case where unlearning immediately follows learning of the forget set—and we consistently see that forgetting is slower in this case.
</figcaption>
      </figure>
    </div>
  </div>
</div>

<!-- Experiments: Shallow vs. Deep Forgetting -->
 <!-- <section id="artifacts" class="py-6 md:py-10 border-y border-slate-200/60 dark:border-slate-800/60">
        <div class="max-w-6xl mx-auto px-4 grid md:grid-cols-2 gap-8 items-center">
          <div>
            <h2 class="text-xl md:text-2xl font-semibold">Artifacts</h2>
            <p class="mt-2 text-slate-600 dark:text-slate-300">Grab the paper and code, and see how to reproduce key plots.</p>
            <div id="get-paper" class="mt-5 flex flex-wrap gap-3">
              <a href="https://arxiv.org/abs/2510.16629" data-href="paper" class="cta inline-flex items-center gap-2 rounded-xl bg-sky-500 px-4 py-2 text-white hover:opacity-90 shadow-soft">📄 <span>Download PDF</span></a>
              <a href="https://github.com/princeton-pli/impossibility-unlearning" data-href="repo" class="cta inline-flex items-center gap-2 rounded-xl border border-slate-200/60 dark:border-slate-800/60 px-4 py-2 hover:shadow-soft">💾 <span>Open repository</span></a>
            </div>
            <ul class="mt-4 text-sm list-disc list-inside text-slate-700 dark:text-slate-200">
              <li>Repro notes and ethics guidance in <code class="font-mono">docs/</code> of the repo.</li>
              <li>Configs in <code class="font-mono">configs/</code>; run script under <code class="font-mono">src/experiments/</code>.</li>
            </ul>
          </div>
          <div class="p-4 md:p-6 rounded-2xl border border-slate-200/60 dark:border-slate-800/60 bg-white/60 dark:bg-slate-900/60 glass">
            <h3 class="text-sm uppercase tracking-wider text-slate-500">Figure placeholder</h3>
            <p class="text-xs text-slate-500">Replace with a teaser plot (e.g., metric vs. unlearning steps for different paths).</p>
            <svg viewBox="0 0 360 180" class="w-full h-40 border border-slate-200/60 dark:border-slate-800/60 rounded-xl mt-2">
              <line x1="30" y1="20" x2="30" y2="150" stroke="currentColor" opacity="0.3"/>
              <line x1="30" y1="150" x2="330" y2="150" stroke="currentColor" opacity="0.3"/>
              <polyline fill="none" stroke="currentColor" stroke-width="2" points="30,140 90,120 150,100 210,85 270,80 330,78"/>
              <polyline fill="none" stroke="var(--accent)" stroke-width="2" points="30,140 90,135 150,130 210,125 270,123 330,122"/>
            </svg>
          </div>
        </div>
      </section> -->

      <section id="bibtex" class="py-6 md:py-10">
        <div class="max-w-6xl mx-auto px-4">
          <h2 class="text-xl md:text-2xl font-semibold">Citation</h2>
          <div class="mt-4 p-4 rounded-2xl border border-slate-200/60 dark:border-slate-800/60 bg-white/60 dark:bg-slate-900/60 glass">
            <div class="flex items-center justify-between gap-2">
              <span class="text-sm opacity-70">BibTeX</span>
              <button id="copy-bib" class="text-xs rounded-lg border border-slate-200/60 dark:border-slate-800/60 px-2 py-1">Copy</button>
            </div>
            <pre class="mt-2 text-xs overflow-auto"><code id="bibtex-code">@article{yu2025impossibility,
  title={On the Impossibility of Retrain Equivalence in Machine Unlearning},
  author={Yu, Jiatong and He, Yinghui and Goyal, Anirudh and Arora, Sanjeev},
  journal={arXiv preprint arXiv:2510.16629,
  year={2025}
}</code></pre>
          </div>
        </div>
      </section>

      <section id="contact" class="py-6 md:py-10">
        <div class="max-w-6xl mx-auto px-4">
          <h2 class="text-xl md:text-2xl font-semibold">Acknowledgments & contact</h2>
          <p class="mt-2 text-sm text-slate-600 dark:text-slate-300">Questions or feedback? Open an issue in the repository or reach out via your preferred channel.</p>
        </div>
      </section>
    </main>

    <footer class="py-8 border-t border-slate-200/60 dark:border-slate-800/60 text-center text-sm">
      <div class="max-w-6xl mx-auto px-4">
        <p>© 2025 The Authors • MIT License • <a class="underline" href="#" data-href="repo">Repository</a></p>
      </div>
    </footer>

    <script>
      // Wire CTAs
      const resolveHref = (el) => {
        const type = el.dataset.href;
        if (type === 'paper') return PAPER_HREF;
        if (type === 'repo') return REPO_URL;
        return '#';
      }
      document.querySelectorAll('a.cta,[data-href]').forEach(a => {
        a.setAttribute('href', resolveHref(a));
        a.setAttribute('target', '_blank');
        a.setAttribute('rel', 'noopener');
      });

      // Theme toggle
      // document.getElementById('theme-toggle').addEventListener('click', () => {
      //   document.documentElement.classList.toggle('dark');
      //   localStorage.setItem('theme', document.documentElement.classList.contains('dark') ? 'dark' : 'light');
      // });

      // Copy BibTeX
      document.getElementById('copy-bib').addEventListener('click', async () => {
        const code = document.getElementById('bibtex-code').textContent.replace('REPO_URL', REPO_URL);
        await navigator.clipboard.writeText(code);
        const btn = document.getElementById('copy-bib');
        const old = btn.textContent; btn.textContent = 'Copied!';
        setTimeout(()=>btn.textContent=old, 1500);
      });

      // Update dynamic links
      document.getElementById('github-cta').setAttribute('href', REPO_URL);
      document.getElementById('primary-cta').setAttribute('href', PAPER_HREF);
    </script>
  </body>
</html>
